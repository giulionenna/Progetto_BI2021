{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "b0c5e0e2f737906a831bf449faf126f39cd636d4f6af7f79c115270e6548b001"
   }
  },
  "interpreter": {
   "hash": "b0c5e0e2f737906a831bf449faf126f39cd636d4f6af7f79c115270e6548b001"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\giuli\\Documents\\GitHub\\Progetto_BI2021\\data.csv', parse_dates=['created_at'])\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "source": [
    "## DECODING DEL TESTO ALL INTERNO DELLA COLONNA `text`. \n",
    "All interno della colonna text è presente una rappresentazione di un bytes sotto forma di stringa. Questa può essere valutata attraverso la funzione `ast.literal_eval(string)` e quindi decodificata\n",
    "#secondo la codifica appropriata tramite il metodo decode"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_enc = []\n",
    "for i in range(0, df.text.size):\n",
    "    txt= ast.literal_eval(df.text[i]).decode('utf-8')\n",
    "    text_enc.append(txt)\n",
    "\n",
    "df['text_enc'] = text_enc\n",
    "df"
   ]
  },
  {
   "source": [
    "## Pulizia dei Tweet\n",
    "Attraverso le Regular Expressions vado a rimuovere elementi come:\n",
    "* @menzioni\n",
    "* Hashtag\n",
    "* Link"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleanTxt(text):\n",
    " text = re.sub('@[A-Za-z0–9]+', '', text) #Rimuove le @menzioni\n",
    " text = re.sub('#', '', text) # Rimuove l'hashtag\n",
    " text = re.sub('https?:\\/\\/\\S+', '', text) # Rimuove i link\n",
    " return text\n",
    "\n",
    "df['text_clean'] = df.text_enc.apply(cleanTxt)\n"
   ]
  },
  {
   "source": [
    "## Language Detection\n",
    "il metodo `detect_langs` fornisce un vettore di possibilità riguardo la lingua del testo che sta analizzando. Vado a vedere se ci sono nel dataset testi ambigui (quindi quelli per cui la dimensione del `dict` ritornato da `detect_langs` è maggiore di 1).\n",
    "\n",
    "Utilizzo `langdetect` che è abbastanza veloce per generare una previsione sommaria. Esso restituisce la probabilità per ciascuna lingua trovata quindi:\n",
    "* se trovo una sola lingua tra quelle ammesse (en, es, fr) allora segno la lingua trovata e segno la detection come sicura\n",
    "* se trovo più di una lingua oppure trovo come lingua più probabile una non ammessa segno la lingua e segno la detection come insicura/sbagliata (`unsure_wrong_detection=True`) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect_langs\n",
    "\n",
    "unsure_count = 0\n",
    "wrong_count = 0\n",
    "i=0\n",
    "lang_detect = []\n",
    "unsure_wrong = []\n",
    "\n",
    "for twt in df.text_enc:\n",
    "    try:\n",
    "        detection = detect_langs(twt)\n",
    "        lang = detection[0].lang\n",
    "        \n",
    "        lang_detect.append(lang)\n",
    "        unsure_wrong.append(False)\n",
    "        \n",
    "        if((lang != \"en\") and (lang != \"es\") and (lang != \"fr\")):\n",
    "             wrong_count = wrong_count +1\n",
    "             unsure_wrong[i]=True\n",
    "        if(len(detection)>1):\n",
    "            unsure_wrong[i]=True\n",
    "            unsure_count = unsure_count+1\n",
    "\n",
    "    except:\n",
    "        print(\"errore alla posizione \")\n",
    "        i\n",
    "        print(\"testo : \" + twt)\n",
    "        lang_detect.append('None')\n",
    "        unsure_wrong.append(True)\n",
    "\n",
    "    i=i+1\n",
    "\n",
    "\n",
    "\n",
    "df['langdetect']=lang_detect\n",
    "df['unsure_wrong_detection'] = unsure_wrong\n",
    "\n",
    "unsure_wrong_sum = df['unsure_wrong_detection'].sum()\n",
    "unsure_wrong_sum"
   ]
  },
  {
   "source": [
    "# Check traduzione attraverso API Google \n",
    "I tweet dei quali la traduzione non è sicura sono stati marcati con un `True` all'interno della colonna `unsure_wrong_detection`. Utilizzo questa informazione per passare i questi tweet all'interno dell'API Google per la traduzione. Questo metodo non è stato utilizzato prima in quanto le possibili chiamate ai server Google sono limitate e molto lente."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "lang_detect_final = []\n",
    "i = 0\n",
    "for twt in df.text_clean:\n",
    "    if not df['unsure_wrong_detection'][i]:\n",
    "        lang_detect_final.append(df['langdetect'][i])\n",
    "    else:\n",
    "        blob = TextBlob(twt)\n",
    "        lang = blob.detect_language()\n",
    "        if((lang != \"en\") and (lang != \"es\") and (lang != \"fr\")):\n",
    "            lang_detect_final.append('None')\n",
    "        else:\n",
    "            lang_detect_final.append(lang)\n",
    "            df['unsure_wrong_detection'][i]=False\n",
    "    i=i+1\n",
    "\n",
    "df['lang_detect_final'] = lang_detect_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.unsure_wrong_detection.sum()"
   ]
  },
  {
   "source": [
    "## Esportazione dei dati su formato Excel\n",
    "Excel non supporta le date con le timezone quindi le ho dovute eliminare con il metodo `datetime.tz_localize()`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc = df[['favorite_count', 'source', 'text_enc', 'text_clean', 'is_retweet', 'retweet_count', 'created_at', 'langdetect', 'unsure_wrong_detection', 'lang_detect_final']]\n",
    "df_enc.loc[:,'created_at_ntz']= df_enc.created_at.dt.tz_localize(None)\n",
    "df_enc.drop(columns='created_at')\n",
    "df_enc = df_enc.drop(columns='created_at')\n",
    "df_enc.to_excel('data_dec.xlsx')"
   ]
  },
  {
   "source": [
    "# Inizio dell'analisi\n",
    "Dopo aver salvato i dati ripuliti all'interno del file Excel, uso questo come checkpoint per partire con l'analisi. Questo perché non è sempre possibile performare di nuovo la traduzione a causa delle restrizioni dell'api di Google descritte sopra\n",
    "\n",
    "Dal momento che i Tweet la cui lingua non è stata riconosciuta sono meno dell' 1%, questi vengono filtrati. Dopodiché il dataset viene splittato per lingua in tre ```DataFrame``` \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel(r'C:\\Users\\giuli\\Documents\\GitHub\\Progetto_BI2021\\data_dec.xlsx')\n",
    "df=pd.DataFrame(data)\n",
    "\n",
    "df = df.drop(columns='langdetect')\n",
    "df.loc[df['unsure_wrong_detection']==True].shape #controllo dei tweet la cui lingua non è stata riconosciuta\n",
    "df = df.loc[df['unsure_wrong_detection']==False] #filtraggio dei tweet non riconosciuti\n",
    "\n",
    "### Suddivisione dei Dataset per lingua ###\n",
    "\n",
    "#en\n",
    "df_en = df.loc[df['lang_detect_final']=='en']\n",
    "df_en = df_en.drop(columns=['lang_detect_final', 'unsure_wrong_detection'])\n",
    "\n",
    "#es\n",
    "df_es = df.loc[df['lang_detect_final']=='es']\n",
    "df_es = df_es.drop(columns=['lang_detect_final', 'unsure_wrong_detection'])\n",
    "\n",
    "#fr\n",
    "df_fr = df.loc[df['lang_detect_final']=='fr']\n",
    "df_fr = df_fr.drop(columns=['lang_detect_final', 'unsure_wrong_detection'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=0.003, stop_words='english')\n",
    "tfidf_en = tfidf.fit_transform(df_en.text_clean.astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3044, 690)"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": []
  }
 ]
}