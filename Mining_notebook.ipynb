{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "b0c5e0e2f737906a831bf449faf126f39cd636d4f6af7f79c115270e6548b001"
   }
  },
  "interpreter": {
   "hash": "96aa8f008242897a655f49e8e2461fc9333f6561e34ec15d9ca2607f5c37f24d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'data.csv', parse_dates=['created_at'])\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "source": [
    "## DECODING DEL TESTO ALL INTERNO DELLA COLONNA `text`. \n",
    "All interno della colonna text è presente una rappresentazione di un bytes sotto forma di stringa. Questa può essere valutata attraverso la funzione `ast.literal_eval(string)` e quindi decodificata\n",
    "#secondo la codifica appropriata tramite il metodo decode"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_enc = []\n",
    "for i in range(0, df.text.size):\n",
    "    txt= ast.literal_eval(df.text[i]).decode('utf-8')\n",
    "    text_enc.append(txt)\n",
    "\n",
    "df['text_enc'] = text_enc\n",
    "df"
   ]
  },
  {
   "source": [
    "## Pulizia dei Tweet\n",
    "Attraverso le Regular Expressions vado a rimuovere elementi come:\n",
    "* @menzioni\n",
    "* Hashtag\n",
    "* Link"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleanTxt(text):\n",
    " text=text.lower()\n",
    " text = re.sub('\\S*trump\\S*', 'trump', text)\n",
    " text = re.sub('\\S*biden\\S*', 'biden', text)\n",
    " text = re.sub('\\S*clinton\\S*', 'clinton', text)\n",
    " text = re.sub('@\\S+', '', text) #Rimuove le @menzioni\n",
    " text = re.sub('#', '', text) # Rimuove l'hashtag\n",
    " text = re.sub('https?:\\/\\/\\S+', '', text) # Rimuove i link\n",
    " text = re.sub('&amp', '', text) #Rimuove &amp\n",
    " text = re.sub('covid-19', 'covid', text ) #converte covid-19 in covid\n",
    " text = re.sub('covid19', 'covid', text ) #converte covid19 in covid\n",
    " text = re.sub('covid 19', 'covid', text ) #converte covid 19 in covid\n",
    " text = re.sub('coronavirus', 'covid', text)\n",
    " return text\n",
    "\n",
    "def countMentions(text):\n",
    "    return len(re.findall('@\\S+', text))\n",
    "\n",
    "def countHashtags(text):\n",
    "    return len(re.findall('#', text))\n",
    "\n",
    "def countLinks(text):\n",
    "    return len(re.findall('https?:\\/\\/\\S+', text))\n",
    "\n",
    "\n",
    "df.text_enc=df.text_enc.astype('str')\n",
    "df['text_clean'] = df.text_enc.apply(cleanTxt)\n",
    "df['mentions'] = df.text_enc.apply(countMentions)\n",
    "df['hashtags'] = df.text_enc.apply(countHashtags)\n",
    "df['links'] = df.text_enc.apply(countLinks)\n"
   ]
  },
  {
   "source": [
    "## Language Detection\n",
    "il metodo `detect_langs` fornisce un vettore di possibilità riguardo la lingua del testo che sta analizzando. Vado a vedere se ci sono nel dataset testi ambigui (quindi quelli per cui la dimensione del `dict` ritornato da `detect_langs` è maggiore di 1).\n",
    "\n",
    "Utilizzo `langdetect` che è abbastanza veloce per generare una previsione sommaria. Esso restituisce la probabilità per ciascuna lingua trovata quindi:\n",
    "* se trovo una sola lingua tra quelle ammesse (en, es, fr) allora segno la lingua trovata e segno la detection come sicura\n",
    "* se trovo più di una lingua oppure trovo come lingua più probabile una non ammessa segno la lingua e segno la detection come insicura/sbagliata (`unsure_wrong_detection=True`) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect_langs\n",
    "\n",
    "unsure_count = 0\n",
    "wrong_count = 0\n",
    "i=0\n",
    "lang_detect = []\n",
    "unsure_wrong = []\n",
    "\n",
    "for twt in df.text_enc:\n",
    "    try:\n",
    "        detection = detect_langs(twt)\n",
    "        lang = detection[0].lang\n",
    "        \n",
    "        lang_detect.append(lang)\n",
    "        unsure_wrong.append(False)\n",
    "        \n",
    "        if((lang != \"en\") and (lang != \"es\") and (lang != \"fr\")):\n",
    "             wrong_count = wrong_count +1\n",
    "             unsure_wrong[i]=True\n",
    "        if(len(detection)>1):\n",
    "            unsure_wrong[i]=True\n",
    "            unsure_count = unsure_count+1\n",
    "\n",
    "    except:\n",
    "        print(\"errore alla posizione \")\n",
    "        i\n",
    "        print(\"testo : \" + twt)\n",
    "        lang_detect.append('None')\n",
    "        unsure_wrong.append(True)\n",
    "\n",
    "    i=i+1\n",
    "\n",
    "\n",
    "\n",
    "df['langdetect']=lang_detect\n",
    "df['unsure_wrong_detection'] = unsure_wrong\n",
    "\n",
    "unsure_wrong_sum = df['unsure_wrong_detection'].sum()\n",
    "unsure_wrong_sum"
   ]
  },
  {
   "source": [
    "# Check traduzione attraverso API Google \n",
    "I tweet dei quali la traduzione non è sicura sono stati marcati con un `True` all'interno della colonna `unsure_wrong_detection`. Utilizzo questa informazione per passare i questi tweet all'interno dell'API Google per la traduzione. Questo metodo non è stato utilizzato prima in quanto le possibili chiamate ai server Google sono limitate e molto lente."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "lang_detect_final = []\n",
    "i = 0\n",
    "for twt in df.text_clean:\n",
    "    if not df['unsure_wrong_detection'][i]:\n",
    "        lang_detect_final.append(df['langdetect'][i])\n",
    "    else:\n",
    "        blob = TextBlob(twt)\n",
    "        lang = blob.detect_language()\n",
    "        if((lang != \"en\") and (lang != \"es\") and (lang != \"fr\")):\n",
    "            lang_detect_final.append('None')\n",
    "        else:\n",
    "            lang_detect_final.append(lang)\n",
    "            df['unsure_wrong_detection'][i]=False\n",
    "    i=i+1\n",
    "\n",
    "df['lang_detect_final'] = lang_detect_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.unsure_wrong_detection.sum() #numero totale di tweet di cui non è stata identificta la lingua"
   ]
  },
  {
   "source": [
    "## Esportazione dei dati su formato Excel\n",
    "Excel non supporta le date con le timezone quindi le ho dovute eliminare con il metodo `datetime.tz_localize()`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc = df[['favorite_count', 'source', 'text_enc', 'text_clean', 'is_retweet', 'retweet_count', 'created_at', 'langdetect' , 'unsure_wrong_detection', 'lang_detect_final', 'mentions', 'hashtags', 'links']]\n",
    "df_enc.loc[:,'created_at_ntz']= df_enc.created_at.dt.tz_localize(None)\n",
    "\n",
    "df_enc = df_enc.drop(columns='created_at')\n",
    "df_enc.to_excel('data_dec.xlsx')"
   ]
  },
  {
   "source": [
    "# Inizio dell'analisi\n",
    "Dopo aver salvato i dati ripuliti all'interno del file Excel, uso questo come checkpoint per partire con l'analisi. Questo perché non è sempre possibile performare di nuovo la traduzione a causa delle restrizioni dell'api di Google descritte sopra\n",
    "\n",
    "Dal momento che i Tweet la cui lingua non è stata riconosciuta sono meno dell' 1%, questi vengono filtrati. Dopodiché il dataset viene splittato per lingua in tre ```DataFrame``` \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel(r'data_dec.xlsx')\n",
    "df=pd.DataFrame(data)\n",
    "\n",
    "df = df.drop(columns='langdetect')\n",
    "df.loc[df['unsure_wrong_detection']==True].shape #controllo dei tweet la cui lingua non è stata riconosciuta\n",
    "df = df.loc[df['unsure_wrong_detection']==False] #filtraggio dei tweet non riconosciuti\n",
    "\n",
    "### Suddivisione dei Dataset per lingua ###\n",
    "\n",
    "#en\n",
    "df_en = df.loc[df['lang_detect_final']=='en']\n",
    "df_en = df_en.drop(columns=['lang_detect_final', 'unsure_wrong_detection'])\n",
    "\n",
    "#es\n",
    "df_es = df.loc[df['lang_detect_final']=='es']\n",
    "df_es = df_es.drop(columns=['lang_detect_final', 'unsure_wrong_detection'])\n",
    "\n",
    "#fr\n",
    "df_fr = df.loc[df['lang_detect_final']=='fr']\n",
    "df_fr = df_fr.drop(columns=['lang_detect_final', 'unsure_wrong_detection'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questo blocco di codice è inutile se si fanno runnare tutti i blocchi dall'inizio\n",
    "\n",
    "df_en.text_enc=df_en.text_enc.astype('str')\n",
    "df_en['text_clean'] = df_en.text_enc.apply(cleanTxt)\n",
    "df_en['mentions'] = df_en.text_enc.apply(countMentions)\n",
    "df_en['hashtags'] = df_en.text_enc.apply(countHashtags)\n",
    "df_en['links'] = df_en.text_enc.apply(countLinks)\n",
    "\n",
    "df_en.to_excel('dataen.xlsx')"
   ]
  },
  {
   "source": [
    "## Preprocessing del testo\n",
    "Attraverso la libreria ```nltk``` viene effettuato il preprocessing del testo. Ogni tweet viene tokenizzato in parole che vengono filtrate dalle stopwords e stemmate attraverso il ```PorterStemmer```. I tweet, una volta stemmati vengono salvati nella colonna text_stem e quindi per comodità de-tokenizzati."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "porter = PorterStemmer()\n",
    "\n",
    "df_en.text_clean=df_en.text_clean.astype('str')\n",
    "\n",
    "text_stem = []\n",
    "#df_en['text_clean']=df_en['text_clean'].astype('str')\n",
    "\n",
    "for twt in df_en.text_clean:\n",
    "    tweet = twt.lower()\n",
    "    token_words = word_tokenize(tweet) #tokenizzazione\n",
    "    token_filter_words = [w for w in token_words if not w in stop_words] #stopword filtering\n",
    "    stem_sentence=[]\n",
    "    for word in token_filter_words:\n",
    "        stem_sentence.append(porter.stem(word)) #stemming\n",
    "        stem_sentence.append(' ')\n",
    "    \n",
    "    text_stem.append(''.join(stem_sentence))\n",
    "\n",
    "df_en['text_stem']=text_stem"
   ]
  },
  {
   "source": [
    "## Tf-Idf\n",
    "A questo punto viene performata la Tf-Idf sui tweet stemmati al punto precedenti. \n",
    "\n",
    "La Tf-idf viene effettuata utilizzando il parametro ```min_df``` che filtra le parole in base alla loro frequenza nel dataset: se ```min_df=0.005``` allora verranno filtrate tutte le parole che compaiono in meno del 0,5% dei tweet. Questo permette di ridurre drasticamente il numero di feature e l'analisi verrà effettuata utilizzando varie percentuali minime. Il parametro ```max_df``` è l'analogo per la frequenza massima. \n",
    "\n",
    "Per comodità di visualizzazione viene creato un dataframe per visualizzare la matrice tf-idf e generare una classifica delle parole: lo score di ciascuna parola è la media del tf-idf della parola in tutti i tweet. Vengono visualizzati i termini con score maggiore."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf(data, mindf,  lan='english'):\n",
    "\n",
    "    tfidfVectorizer = TfidfVectorizer(min_df=mindf,\n",
    "                                    stop_words=lan)\n",
    "    tfidf_matrix = tfidfVectorizer.fit_transform(data['text_stem'])\n",
    "\n",
    "    print(\"sono state prodotte\", len(tfidfVectorizer.get_feature_names()), \"parole nel processo di tf-idf con mindf=\", mindf)\n",
    "    ## Conversione TfIdf in Dataframe ##\n",
    "\n",
    "    # tfidfVectorizer ritorna una matrice sparsa che non permette la visualizzazione agevole della matrice TfIdf\n",
    "\n",
    "    tf_idf = pd.DataFrame(columns=tfidfVectorizer.get_feature_names(), index=data['text_clean'], dtype=float)\n",
    "    \n",
    "    M=tfidf_matrix.todense()\n",
    "    for i in range(0, data.shape[0]):\n",
    "        tf_idf.iloc[i, :]=M[i,:]\n",
    "\n",
    "    ## classifica delle parole\n",
    "    #score={}\n",
    "    #for word in tfidfVectorizer.get_feature_names():\n",
    "    #    score[word]= tf_idf_en[word].mean()\n",
    "\n",
    "    #score_sroted = dict(sorted(score.items(), key=lambda item: item[1], reverse=True))\n",
    "    #print(\"Score delle prime 10 parole: \", list(score_sroted.items())[0:10])\n",
    "\n",
    "    return tf_idf, M\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Scelta del numero di feature\n",
    "Proseguo l'analisi con una mindf di 0.001, 0.005 e 0.01"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_en_01, tfidf_en_matrix_01 = tfidf(df_en, 0.001)\n",
    "tf_idf_en_05, tfidf_en_matrix_05 = tfidf(df_en, 0.005)\n",
    "tf_idf_en_10, tfidf_en_matrix_10 = tfidf(df_en, 0.01)\n",
    "tf_idf_en_00, tfidf_en_matrix_00 = tfidf(df_en, 0)"
   ]
  },
  {
   "source": [
    "## PCA vs ```min_df```\n",
    "Provo a ridurre le features non alla cieca come il parametro ```min_df``` della Tf Idf ma preservando le informazioni all'interno del dataset.\n",
    "La PCA è giustificata dalla non necessità di interpretare le feature."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "def feature_redux(M):\n",
    "    original_size=M.shape[1]\n",
    "    #Standardizzo i dati\n",
    "    scaler = StandardScaler()\n",
    "    M_redux = scaler.fit_transform(M)\n",
    "\n",
    "    #Applico la PCA mantenendo il 95% della varianza\n",
    "    pca=PCA(0.90)\n",
    "    M_redux = pca.fit_transform(M_redux)\n",
    "\n",
    "    print(\"Il numero di feature è stato ridotto da \",original_size,\"a\", pca.n_components_)\n",
    "\n",
    "    #scatter 3d dei punti prodotti\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    from matplotlib import pyplot\n",
    "\n",
    "    figure = pyplot.figure()\n",
    "    ax = Axes3D(figure)\n",
    "\n",
    "    ax.scatter(M_redux[:,0], M_redux[:,1], M_redux[:,2])\n",
    "    pca.explained_variance_\n",
    "    pyplot.show()\n",
    "\n",
    "    return M_redux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_en_matrix_PCA = feature_redux(tfidf_en_matrix_00)"
   ]
  },
  {
   "source": [
    "# K-means Clustering\n",
    "Viene performato un algoritmo di K-means Clustering, valutando i valori di K e plottando la Sum of Square Differences."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "def kmeans_ssd_plot(M):\n",
    "    SSD = []\n",
    "    K=range(2,30)\n",
    "    t=time.perf_counter()\n",
    "    for k in K:\n",
    "        print(\"Kmeans clustering con k = \", k)\n",
    "        km=KMeans(n_clusters=k, max_iter=300, n_init=20)\n",
    "        km = km.fit(M)\n",
    "        SSD.append(km.inertia_)\n",
    "    print(\"valutazione del kmeans clustering terminata in\", time.perf_counter()-t)\n",
    "    plt.plot(K, SSD, 'bx-')\n",
    "    return SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSD = kmeans_ssd_plot(tfidf_en_matrix_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_ssd_plot(tfidf_en_matrix_05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSD = kmeans_ssd_plot(tfidf_en_matrix_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSD = kmeans_ssd_plot(tfidf_en_matrix_00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSD = kmeans_ssd_plot(tfidf_en_matrix_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSD"
   ]
  },
  {
   "source": [
    "Si sceglie un valore di k=15, si performa il clustering e si aggiunge l'informazione di labeling al dataframe.\n",
    "Eventualmente sarà possibile unificare i cluster simili tra loro confrontandoli con un algoritmo custom."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=15\n",
    "\n",
    "model = KMeans(n_clusters=k, max_iter=300, n_init=20)\n",
    "model.fit(tfidf_en_matrix_01)\n",
    "df_en['label_kmeans_01']=model.labels_\n",
    "\n",
    "model = KMeans(n_clusters=k, max_iter=300, n_init=20)\n",
    "model.fit(tfidf_en_matrix_05)\n",
    "df_en['label_kmeans_05']=model.labels_\n",
    "\n",
    "model = KMeans(n_clusters=k, max_iter=300, n_init=20)\n",
    "model.fit(tfidf_en_matrix_10)\n",
    "df_en['label_kmeans_10']=model.labels_\n",
    "\n",
    "model = KMeans(n_clusters=k, max_iter=300, n_init=20)\n",
    "model.fit(tfidf_en_matrix_PCA)\n",
    "df_en['label_kmeans_PCA']=model.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def cluster_eval(X, k):\n",
    "    #++++++++++PER CLUSTER SSE+++++++++++++++++++\n",
    "    model = KMeans(n_clusters=k, max_iter=300, n_init=20, random_state=3425).fit(X)\n",
    "    cluster_centers = [X[model.labels_ == i].mean(axis=0) for i in range(k)]\n",
    "\n",
    "    clusterwise_sse= [0 for x in range(k)] #VETTORE DI SSE PER OGNI CLUSTER\n",
    "    for point, label in zip(X, model.labels_):\n",
    "        clusterwise_sse[label] += np.square(point - cluster_centers[label]).sum()\n",
    "\n",
    "    #-------------SILHOUETTE-----------------------\n",
    "    from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "\n",
    "    silhouette_avg = silhouette_score(X, model.labels_)\n",
    "    sample_silhouette_values = silhouette_samples(X, model.labels_)\n",
    "\n",
    "    print(\"La clusterizzazione ha prodotto cluster con coefficiente di silhouette pari a\", silhouette_avg)\n",
    "\n",
    "    import matplotlib.cm as cm\n",
    "    \n",
    "\n",
    "    fig, (ax1) = plt.subplots(1)\n",
    "    fig.set_size_inches(10,10)\n",
    "    ax1.set_xlim([-0.1, 0.3])\n",
    "    ax1.set_ylim([0, len(X) + (k + 1) * 10])\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(k):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[model.labels_ == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / k)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                            0, ith_cluster_silhouette_values,\n",
    "                            facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"Plot della Silhouette per i vari cluster\")\n",
    "    ax1.set_xlabel(\"Valori di silhouette\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    #ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                    \"with n_clusters = %d\" % k),\n",
    "                    fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(18,7)\n",
    "    ax.bar(range(k), clusterwise_sse)\n",
    "    ax.set_xlabel('Cluster')\n",
    "    ax.set_ylabel('SSE')\n",
    "    ax.set_title('SSE per cluster')\n",
    "    plt.show()\n",
    "    return sample_silhouette_values, clusterwise_sse\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_eval(tfidf_en_matrix_01, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Test regole di associazione\n",
    "viene testato l'algoritmo apriori sulla versione onehot encoded della tfidf del dataset. La colonna \"covid\" viene droppata in quanto estremamente frequente e quindi non informativa"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = tf_idf_en_00\n",
    "M=M.drop('covid', axis='columns')\n",
    "def onehot(r):\n",
    "    if r==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "M = M.applymap(onehot)\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "freq_items = apriori(M, min_support=0.005, use_colnames=True)\n",
    "rules = association_rules(freq_items, metric='confidence', min_threshold=0.3)\n",
    "rules\n",
    "\n",
    "rules.to_excel('rules.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules"
   ]
  },
  {
   "source": [
    "## VAlutazione qualitativa dei cluster utilizzando una Wordcloud\n",
    "\n",
    "Questo è il codice per generare Wordcloud\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "def cloud(df, label, ncluster, text_index='text_clean'):\n",
    "\n",
    "    frequent_words=['covid', 're']\n",
    "    \n",
    "\n",
    "    df_0 = df.loc[df[label]==ncluster]\n",
    "    text = []\n",
    "    for twt in df_0[text_index]:\n",
    "        token_words = word_tokenize(twt) #tokenizzazione\n",
    "        token_filter_words_1 = [w for w in token_words if not w in stop_words]\n",
    "        token_filter_words_2 = [w for w in token_filter_words_1 if not w in frequent_words] #stopword filtering\n",
    "        for w in token_filter_words_2:\n",
    "            text.append(w)\n",
    "    \n",
    "    \n",
    "    txt=' '.join([word for word in text])\n",
    "    wordcloud = WordCloud(max_font_size=300, max_words=100, background_color=\"white\", width=1920, height=1080).generate(txt)\n",
    "    fig1 = plt.figure(figsize=(6,6))\n",
    "    fig1.set_size_inches(18, 7)\n",
    "    plt.imshow(wordcloud)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=15\n",
    "\n",
    "model = KMeans(n_clusters=k, max_iter=300, n_init=10, random_state=5)\n",
    "#random state precedente = 5\n",
    "model.fit(tfidf_en_matrix_01)\n",
    "df_en['label_kmeans_01']=model.labels_\n",
    "\n",
    "for i in range(k):\n",
    "    cloud(df_en, 'label_kmeans_01', i)\n"
   ]
  },
  {
   "source": [
    "Risultati di questo clustering:\n",
    "\n",
    "* (2)-scuole\n",
    "* (5)-vaccini\n",
    "* (11)-texas\n",
    "\n",
    "Codice per consolidare questo clustering:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en.to_excel('df_en_clustered.xlsx')\n"
   ]
  },
  {
   "source": [
    "## Regole di associazione associate a ciascun cluster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20516/3791493343.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_en\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'df_en_clustered.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_en=pd.DataFrame(pd.read_excel('df_en_clustered.xlsx'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17144/1875990779.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mM_school\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_en\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_kmeans_01\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mrul\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM_school\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17144/1875990779.py\u001b[0m in \u001b[0;36mrules\u001b[1;34m(M, min_sup)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrequent_patterns\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapriori\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0massociation_rules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_sup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mfreq_items\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapriori\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_support\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_sup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_colnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mrules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0massociation_rules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'confidence'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mlxtend\\frequent_patterns\\apriori.py\u001b[0m in \u001b[0;36mapriori\u001b[1;34m(df, min_support, use_colnames, max_len, verbose, low_memory)\u001b[0m\n\u001b[0;32m    269\u001b[0m                 itemset_dict[max_itemset], X, min_support, is_sparse)\n\u001b[0;32m    270\u001b[0m             \u001b[1;31m# slightly faster than creating an array from a list of tuples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m             \u001b[0mcombin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromiter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m             \u001b[0mcombin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_max_itemset\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mlxtend\\frequent_patterns\\apriori.py\u001b[0m in \u001b[0;36mgenerate_new_combinations_low_memory\u001b[1;34m(old_combinations, X, min_support, is_sparse)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mmask_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_tuple\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m             \u001b[0msupports\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask_rows\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_items\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m         \u001b[0mvalid_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msupports\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalid_indices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#genero una matrice one-hot encoded a partire dalla tf-idf completa\n",
    "M=tf_idf_en_00\n",
    "M=M.drop('covid', axis='columns')\n",
    "\n",
    "def onehot(r):\n",
    "    if r==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "def rules(M, min_sup=0.005):\n",
    "    freq_items = apriori(M, min_support=min_sup, use_colnames=True, low_memory=True)\n",
    "    rules = association_rules(freq_items, metric='confidence', min_threshold=0.3)\n",
    "    return rules\n",
    "\n",
    "\n",
    "\n",
    "M=M.applymap(onehot)\n",
    "M=M.reset_index(drop=True)\n",
    "M_school=M[df_en.label_kmeans_01==2]\n",
    "\n",
    "rul=rules(M_school)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_school=M_school.reset_index(drop=True)\n",
    "M_school"
   ]
  },
  {
   "source": [
    "## Esaminazione dei cluster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizzazione dei centroidi tramite PCA\n",
    "\n",
    "M = model.cluster_centers_\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "#Standardizzo i dati\n",
    "scaler = StandardScaler()\n",
    "M = scaler.fit_transform(M)\n",
    "\n",
    "#Applico la PCA mantenendo le prime 3 componenti\n",
    "pca=PCA(n_components=3)\n",
    "M = pca.fit_transform(M)\n",
    "\n",
    "#scatter 3d dei centroidi prodotti\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot\n",
    "\n",
    "figure = pyplot.figure()\n",
    "ax = Axes3D(figure)\n",
    "\n",
    "ax.scatter(M[:,0], M[:,1], M[:,2])\n",
    "pca.explained_variance_\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "source": [
    "# CODICE SPERIMENTALE - IGNORA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Utilizo del DB-SCAN per raggruppare i cluster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "M= model.cluster_centers_\n",
    "pca=PCA(0.95)\n",
    "M= pca.fit_transform(scaler.fit_transform(M))\n",
    "\n",
    "clustering = DBSCAN(eps=1, metric='cosine').fit(model.cluster_centers_)\n",
    "clustering.labels_"
   ]
  },
  {
   "source": [
    "# DB Scan Clustering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "M=scaler.fit_transform(tfidf_en_matrix_00)\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=5, radius=20, n_jobs=-1).fit(M)\n",
    "distances, indices = nbrs.kneighbors(M)\n",
    "distances_plot = [d[4] for d in distances ]\n",
    "#print(indices[0],distances[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_plot.sort()\n",
    "plt.plot(range(M.shape[0]), distances_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "clustering = DBSCAN(eps=100, min_samples=5).fit(M)\n",
    "\n",
    "\n",
    "df_en['labels_DBSCAN']= clustering.labels_\n",
    "df_en.loc[df_en['labels_DBSCAN']==0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en.loc[df_en['labels_DBSCAN']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud(df_en, 'labels_DBSCAN', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_en.reset_index(drop=True, inplace=True )\n",
    "df_en.reset_index(drop=True, inplace=True)\n",
    "df_en_concat = pd.concat([df_en, tf_idf_en], axis=1)\n",
    "df_en_concat.to_excel('df_en_concat.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_en.to_excel('tf_idf_en.xlsx')"
   ]
  }
 ]
}